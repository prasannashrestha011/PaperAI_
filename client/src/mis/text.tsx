export const testResult=`
This paper presents a novel Discrete-Action Deep Q-Network (DQN) approach for scheduling Energy Storage Systems (ESS) in residential households with the primary objective of minimizing energy costs [^1]. The research addresse
s the growing demand for electric energy and the limitations of clean sources, highlighting the importance of demand-side energy management to ensure sustainable growth and avoid power system instability [^2, ^3,^4].            
                                                                                                                                                                                                                                 
The core problem tackled is the inherent complexity of optimizing energy scheduling due to factors like uncertain energy demand, renewable generation, large system scale, and conflicting economic and technical requirements [^
4]. Traditional approaches like market-based demand response programs often require active user participation, which can be inconvenient [^5]. The paper proposes integrating Distributed Energy Resources (DER) and ESS with an 
Energy Management System (EMS) at the household level to overcome this, allowing users to efficiently consume energy and potentially generate income by optimizing energy import/export based on price fluctuations [^6].        
                                                                                                                                                                                                                                 
The proposed methodology formulates the energy scheduling problem as a Markov Decision Process (MDP) from the user's perspective, aiming to find the optimal hour-ahead energy schedule to minimize costs [^7]. The system state 
at each time step includes the difference between predicted generation and demand (surplus energy), the current State of Charge (SoC) of the battery, and the import and export prices for the next hour [^8]. The action space i
s discretized into 9 possible levels, ranging from -1 (maximum export/discharging) to 1 (maximum import/charging), including intermediate steps like -0.75, -0.5, 0, 0.5, etc. [^9, ^10]. This normalization of the state and dis
cretization of actions are key to enabling effective state representation and decision-making [^11].                                                                                                                             
                                                                                                                                                                                                                                 
A reward function is defined, consisting of two components: the reward from the cost of energy imported or exported (negative for import, positive for export) and a punishment for unbalance in the system (e.g., importing at f
ull SoC or exporting at zero surplus/SoC) [^12, ^13]. The Deep Reinforcement Learning (DRL) framework utilizes a neural network as a function approximator for the Q-value function, which maps state-action pairs to expected re
wards [^14]. The learning algorithm employs an epsilon-greedy policy to balance exploration and exploitation, stores state transitions in an experience replay memory, and updates the neural network by minimizing the mean squa
re error loss using the Bellman equation [^15, ^16].                                                                                                                                                                             
                                                                                                                                                                                                                                 
Simulation results validate the superiority of the proposed discrete-action DQN method over several other approaches:                                                                                                            
*   It achieves cost savings of 43% compared to a baseline with no rule-based strategy [^17].                                                                                                                                    
*   It demonstrates 21% more cost savings than the average rule-based approach [^18].                                                                                                                                            
*   It shows an 11% improvement in cost efficiency over a three-level scheduling DQN method, which uses a limited action space of charging, discharging, or remaining idle [^19, ^20].                                           
                                                                                                                                                                                                                                 
The paper concludes that the finer discretization of actions significantly enhances cost efficiency and underscores the potential of this approach to improve energy management in smart grids for residential households [^21, ^
22]. Future work is suggested to extend the model to handle the stochastic nature of real-world energy loads and generation, accounting for forecasting uncertainties to enhance robustness and practical applicability [^23].
`

export const testCitations = [
  "This paper presents a Discrete-Action Deep Q-Network (DQN) approach for scheduling Energy Storage Systems (ESS) to optimize energy costs for residential households.",
  "With growing demand for electric energy, and limitations on clean sources of energy, managing energy on the demand side has played a significant role in enabling sustainable growth of energy systems [1, 2].",
  "Energy demand reduction or shifting from peak to off-peak period has played a significant role in avoiding instability in the power system and avoiding situations leading to blackouts.",
  "Solving for optimum energy scheduling is inherently complex due to the interplay of several factors such as uncertainty of energy demand and renewable generation, the large-scale nature of the system, conflicting economic, strict technical and regulatory requirements.",
  "However, from the perspective of the user, it can be inconvenient, as shifting or shedding of energy usage requires their active participation to maintain economic benefits.",
  "A common approach to overcome this issue has been the integration of Distributed Energy Resources (DER), such as solar and ESS with an energy management system (EMS) on individual households [6].",
  "In this paper, the energy scheduling problem for an individual household is formulated as an Markov Decision Process (MDP) from a user perspective. The objective is to find the optimal scheduling of an hour ahead energy, to minimize the cost of energy utilized based on the varying price of the energy throughout the day.",
  "State definition: The system state at time step t is represented as a vector S_t = {S_t^{surplus}, S_t^{SoC}, P_t^{import}, P_t^{export}}. The vector encapsulates 4 different pieces of information: S_t^{surplus} indicates the difference in predicted generation and demand, S_t^{surplus} = G_t^{pred} - D_t^{pred}; S_t^{SoC} is the current state of charge of the battery; and P_t^{import}, P_t^{export} represented the import and export price of the energy for the next hour, here the price can be a predicted real-time price or time of use (ToU) tariff.",
  "Action Space: Based on the state S_t, the a_t represents the decision to import or export energy for the next hour. The user can make money by importing the energy at a lower import price and the export or using battery when the import price is high. Action taken can be constrained as -P_max^{export} <= a_t <= P_max^{import}, where -P_max^{export} and P_max^{import} are the maximum allowable energy export and import, which is the maximum charging or discharging from the battery, respectively. As suggested by [14], the action is discretized into predefined steps.",
  "The input to the proposed architecture of the RL model has 9 possible actions which range from -1 to 1, such that the action space is normalized, however the action can range to n different actions that can be taken, and the total action steps can be written as: A = {a_1, a_2, a_3 ... a_n}.",
  "The proposed model normalizes the state based on the maximum allowable actions defined by the policy, enabling effective state representation and decision-making.",
  "Reward Function: The reward R_t at each time step t consists of two components are defined as, R_t = R_{cost} + R_{unbalance}."
];
